#!/usr/bin/env python3
"""
Complete RWKV-7 "Goose" Implementation with Robust Dataset Loading
Designed for Google Colab with A100 GPU
"""

# ============================================================================
# PART 1: IMPORTS AND SETUP
# ============================================================================

print("=" * 70)
print("RWKV-7 'Goose' Complete Training Pipeline")
print("=" * 70)

# Standard imports
import os
import sys
import math
import json
import random
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass

# PyTorch imports
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.cuda.amp import autocast, GradScaler

# Transformers imports
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup
except ImportError:
    print("Installing transformers...")
    os.system("pip install transformers")
    from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup

# Datasets imports
try:
    from datasets import load_dataset
except ImportError:
    print("Installing datasets...")
    os.system("pip install datasets")
    from datasets import load_dataset

# Progress bar
try:
    from tqdm import tqdm
except ImportError:
    print("Installing tqdm...")
    os.system("pip install tqdm")
    from tqdm import tqdm

# Check device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nDevice: {DEVICE}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("WARNING: No GPU available, training will be slow!")

# ============================================================================
# PART 2: RWKV-7 MODEL IMPLEMENTATION
# ============================================================================

class RWKV7TimeMixingModule(nn.Module):
    """Core RWKV-7 Time-Mixing mechanism with matrix-valued states"""

    def __init__(self, hidden_size: int, num_heads: int):
        super().__init__()
        assert hidden_size % num_heads == 0, f"hidden_size ({hidden_size}) must be divisible by num_heads ({num_heads})"

        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_size = hidden_size // num_heads

        # Linear projections
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.r_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.w_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.a_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.output_proj = nn.Linear(hidden_size, hidden_size, bias=False)

        # Learnable parameters
        self.kappa = nn.Parameter(torch.randn(1, self.num_heads, 1, 1) * 0.01)
        self.mu = nn.Parameter(torch.randn(1, 1, self.hidden_size) * 0.01)

    def forward(self, x: torch.Tensor, state: torch.Tensor, last_x: torch.Tensor):
        B, T, C = x.shape
        H = self.num_heads
        N = self.head_size

        # Token-shift mixing
        if T > 1:
            shifted_x = torch.cat([last_x, x[:, :-1]], dim=1)
        else:
            shifted_x = last_x
        x_mixed = torch.lerp(x, shifted_x, torch.sigmoid(self.mu))

        # Projections
        k = self.k_proj(x_mixed).view(B, T, H, N)
        v = self.v_proj(x_mixed).view(B, T, H, N)
        r = self.r_proj(x_mixed).view(B, T, H, N).transpose(1, 2)
        w = torch.sigmoid(self.w_proj(x_mixed)).transpose(1, 2)
        a = torch.sigmoid(self.a_proj(x_mixed)).transpose(1, 2)

        # Sequential processing
        outputs = []
        for t in range(T):
            kt = k[:, t, :, :]
            vt = v[:, t, :, :]
            wt = w[:, :, t].view(B, H, N)
            at = a[:, :, t].view(B, H, N)
            rt = r[:, :, t, :]

            # State update
            vt_vec = vt.unsqueeze(3)
            kt_vec = kt.unsqueeze(3)
            addition_term = vt_vec @ kt_vec.transpose(2, 3)

            at_vec = at.unsqueeze(3)
            removal_term = self.kappa * (at_vec @ kt_vec.transpose(2, 3))

            state = state * torch.diag_embed(wt) - removal_term + addition_term

            # Output
            output_t = rt.unsqueeze(2) @ state
            outputs.append(output_t.squeeze(2))

        output = torch.stack(outputs, dim=1).transpose(1, 2).reshape(B, T, C)
        updated_last_x = x[:, -1:, :]

        return self.output_proj(output), state, updated_last_x


class RWKV7StudentModel(nn.Module):
    """Complete RWKV-7 'Goose' architecture"""

    def __init__(self, vocab_size: int, hidden_size: int, num_layers: int, num_heads: int):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_heads = num_heads

        # Token embeddings
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.ln_in = nn.LayerNorm(hidden_size)

        # RWKV blocks
        self.blocks = nn.ModuleList()
        for _ in range(num_layers):
            block = nn.ModuleDict({
                'ln1': nn.LayerNorm(hidden_size),
                'att': RWKV7TimeMixingModule(hidden_size, num_heads),
                'ln2': nn.LayerNorm(hidden_size),
                'ffn': nn.Sequential(
                    nn.Linear(hidden_size, 4 * hidden_size),
                    nn.GELU(),
                    nn.Linear(4 * hidden_size, hidden_size)
                )
            })
            self.blocks.append(block)

        # Output head
        self.ln_out = nn.LayerNorm(hidden_size)
        self.output_head = nn.Linear(hidden_size, vocab_size, bias=False)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Custom weight initialization"""
        nn.init.uniform_(self.embedding.weight, -1e-4, 1e-4)

        for module in self.modules():
            if isinstance(module, nn.Linear):
                if 'output' in str(module) or 'ffn.2' in str(module):
                    nn.init.zeros_(module.weight)
                else:
                    nn.init.normal_(module.weight, std=0.02)

                if hasattr(module, 'bias') and module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, input_ids: torch.Tensor, states: Optional[List] = None):
        B, T = input_ids.shape
        H, N = self.num_heads, self.hidden_size // self.num_heads

        # Initialize states if needed
        if states is None:
            states = []
            for _ in range(self.num_layers):
                wkv_state = torch.zeros(B, H, N, N, device=input_ids.device, dtype=torch.float32)
                last_x = torch.zeros(B, 1, self.hidden_size, device=input_ids.device, dtype=torch.float32)
                states.append((wkv_state, last_x))

        # Token embeddings
        x = self.embedding(input_ids)
        x = self.ln_in(x)

        # Process through blocks
        new_states = []
        for i, block in enumerate(self.blocks):
            wkv_state, last_x = states[i]

            # Attention
            x_norm = block['ln1'](x)
            dx, new_wkv_state, new_last_x = block['att'](x_norm, wkv_state, last_x)
            x = x + dx

            # FFN
            x_norm = block['ln2'](x)
            dx = block['ffn'](x_norm)
            x = x + dx

            new_states.append((new_wkv_state.detach(), new_last_x.detach()))

        # Output projection
        x = self.ln_out(x)
        logits = self.output_head(x)

        return logits, new_states


# ============================================================================
# PART 3: DATASET HANDLING
# ============================================================================

class SimpleTextDataset(Dataset):
    """Simple text dataset wrapper"""

    def __init__(self, texts: List[str]):
        self.texts = texts

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx]


def load_and_prepare_dataset(dataset_name: str = "tatsu-lab/alpaca", max_samples: Optional[int] = None):
    """Load and prepare a dataset with proper error handling"""

    print(f"\n{'='*60}")
    print(f"Loading dataset: {dataset_name}")
    print(f"{'='*60}")

    try:
        # Try loading the dataset
        if dataset_name == "tatsu-lab/alpaca":
            # Alpaca dataset - instruction following
            dataset = load_dataset(dataset_name, split="train")

            print(f"Dataset loaded successfully!")
            print(f"Total samples: {len(dataset)}")
            print(f"Columns: {dataset.column_names}")

            # Process Alpaca format
            texts = []
            for sample in tqdm(dataset.select(range(min(max_samples or len(dataset), len(dataset)))),
                             desc="Processing samples"):
                instruction = sample.get('instruction', '')
                input_text = sample.get('input', '')
                output = sample.get('output', '')

                if input_text:
                    text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
                else:
                    text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

                texts.append(text)

        elif dataset_name == "roneneldan/TinyStories":
            # TinyStories - simple narratives
            dataset = load_dataset(dataset_name, split="train")

            print(f"Dataset loaded successfully!")
            print(f"Total samples: {len(dataset)}")

            texts = []
            for sample in tqdm(dataset.select(range(min(max_samples or len(dataset), len(dataset)))),
                             desc="Processing samples"):
                texts.append(sample['text'])

        else:
            # Generic handling
            dataset = load_dataset(dataset_name, split="train")
            print(f"Dataset loaded successfully!")
            print(f"Columns: {dataset.column_names}")

            # Try to find text column
            text_column = None
            for col in ['text', 'content', 'document', 'passage']:
                if col in dataset.column_names:
                    text_column = col
                    break

            if text_column:
                texts = [sample[text_column] for sample in dataset.select(range(min(max_samples or len(dataset), len(dataset))))]
            else:
                print(f"WARNING: Could not find text column. Using first column: {dataset.column_names[0]}")
                texts = [str(sample[dataset.column_names[0]]) for sample in dataset.select(range(min(max_samples or len(dataset), len(dataset))))]

        print(f"\nProcessed {len(texts)} samples")
        print(f"Sample text preview:\n{texts[0][:200]}...\n")

        return SimpleTextDataset(texts)

    except Exception as e:
        print(f"ERROR loading dataset: {e}")
        print("Falling back to dummy data...")

        # Fallback dummy data
        dummy_texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "Python is a popular programming language.",
            "Neural networks can learn complex patterns.",
            "Deep learning has revolutionized AI.",
        ] * 100

        return SimpleTextDataset(dummy_texts)


# ============================================================================
# PART 4: DISTILLATION TRAINER
# ============================================================================

class DistillationTrainer:
    """Knowledge distillation trainer with error handling"""

    def __init__(self, teacher_model_name: str, student_config: Dict, device: torch.device):
        self.device = device

        print(f"\nInitializing DistillationTrainer...")
        print(f"Teacher model: {teacher_model_name}")

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print("Set pad_token = eos_token")

        # Load teacher model
        print("Loading teacher model...")
        self.teacher_model = AutoModelForCausalLM.from_pretrained(
            teacher_model_name,
            torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32
        ).to(device)
        self.teacher_model.eval()

        # Get vocab size from teacher
        self.vocab_size = self.teacher_model.config.vocab_size
        print(f"Vocabulary size: {self.vocab_size}")

        # Initialize student model
        print("Initializing student model...")
        self.student_model = RWKV7StudentModel(
            vocab_size=self.vocab_size,
            hidden_size=student_config['hidden_size'],
            num_layers=student_config['num_layers'],
            num_heads=student_config['num_heads']
        ).to(device)

        print(f"Student model parameters: {sum(p.numel() for p in self.student_model.parameters()) / 1e6:.2f}M")

    def train(self, dataset: Dataset, num_epochs: int = 2, batch_size: int = 4,
              lr: float = 5e-4, max_length: int = 256, gradient_accumulation_steps: int = 1,
              checkpoint_dir: Optional[str] = None):
        """Train the student model with distillation"""

        print(f"\n{'='*60}")
        print("Training Configuration:")
        print(f"{'='*60}")
        print(f"Epochs: {num_epochs}")
        print(f"Batch size: {batch_size}")
        print(f"Learning rate: {lr}")
        print(f"Max sequence length: {max_length}")
        print(f"Gradient accumulation: {gradient_accumulation_steps}")
        print(f"Effective batch size: {batch_size * gradient_accumulation_steps}")

        # Create dataloader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            drop_last=True
        )

        # Optimizer and scheduler
        optimizer = optim.AdamW(self.student_model.parameters(), lr=lr, weight_decay=0.01)
        num_training_steps = len(dataloader) * num_epochs // gradient_accumulation_steps
        num_warmup_steps = int(num_training_steps * 0.1)

        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )

        # Mixed precision scaler
        scaler = GradScaler() if self.device.type == 'cuda' else None

        # Training loop
        self.student_model.train()
        global_step = 0

        for epoch in range(num_epochs):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch + 1}/{num_epochs}")
            print(f"{'='*60}")

            epoch_loss = 0
            progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}")

            for batch_idx, batch in enumerate(progress_bar):
                # Tokenize batch
                try:
                    encoded = self.tokenizer(
                        batch,
                        return_tensors='pt',
                        padding=True,
                        truncation=True,
                        max_length=max_length
                    )
                except Exception as e:
                    print(f"Tokenization error: {e}")
                    continue

                input_ids = encoded.input_ids.to(self.device)
                attention_mask = encoded.attention_mask.to(self.device)

                # Skip if sequence too short
                if input_ids.shape[1] < 2:
                    continue

                # Get teacher outputs
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(
                        input_ids,
                        attention_mask=attention_mask
                    )
                    teacher_logits = teacher_outputs.logits

                # Forward pass with mixed precision
                if scaler:
                    with autocast():
                        student_logits, _ = self.student_model(input_ids)
                        loss = self._compute_loss(
                            student_logits,
                            teacher_logits,
                            attention_mask
                        )
                else:
                    student_logits, _ = self.student_model(input_ids)
                    loss = self._compute_loss(
                        student_logits,
                        teacher_logits,
                        attention_mask
                    )

                # Scale loss for gradient accumulation
                loss = loss / gradient_accumulation_steps

                # Backward pass
                if scaler:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

                # Update weights
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    if scaler:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)
                        optimizer.step()

                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1

                # Update metrics
                epoch_loss += loss.item() * gradient_accumulation_steps
                avg_loss = epoch_loss / (batch_idx + 1)

                progress_bar.set_postfix({
                    'loss': f'{avg_loss:.4f}',
                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'
                })

                # Save checkpoint
                if checkpoint_dir and global_step % 500 == 0:
                    os.makedirs(checkpoint_dir, exist_ok=True)
                    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{global_step}.pt')
                    torch.save(self.student_model.state_dict(), checkpoint_path)
                    print(f"\nSaved checkpoint: {checkpoint_path}")

            # End of epoch
            avg_epoch_loss = epoch_loss / len(dataloader)
            print(f"\nEpoch {epoch + 1} completed. Average loss: {avg_epoch_loss:.4f}")

            # Save epoch checkpoint
            if checkpoint_dir:
                os.makedirs(checkpoint_dir, exist_ok=True)
                checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch + 1}.pt')
                torch.save(self.student_model.state_dict(), checkpoint_path)
                print(f"Saved epoch checkpoint: {checkpoint_path}")

        return self.student_model

    def _compute_loss(self, student_logits: torch.Tensor, teacher_logits: torch.Tensor,
                      attention_mask: torch.Tensor, temperature: float = 4.0):
        """Compute distillation loss"""

        # Shift for language modeling
        shift_logits = student_logits[..., :-1, :].contiguous()
        shift_teacher = teacher_logits[..., :-1, :].contiguous()
        shift_mask = attention_mask[..., 1:].contiguous()

        # Handle vocabulary size mismatch
        vocab_size = min(shift_logits.shape[-1], shift_teacher.shape[-1])
        shift_logits = shift_logits[..., :vocab_size]
        shift_teacher = shift_teacher[..., :vocab_size]

        # Flatten and apply mask
        shift_logits = shift_logits.view(-1, vocab_size)
        shift_teacher = shift_teacher.view(-1, vocab_size)
        shift_mask = shift_mask.view(-1).bool()

        if shift_mask.sum() == 0:
            return torch.tensor(0.0, device=shift_logits.device)

        masked_logits = shift_logits[shift_mask]
        masked_teacher = shift_teacher[shift_mask]

        # KL divergence loss
        kl_loss = F.kl_div(
            F.log_softmax(masked_logits / temperature, dim=-1),
            F.softmax(masked_teacher / temperature, dim=-1),
            reduction='batchmean'
        ) * (temperature ** 2)

        # Hard label loss
        hard_labels = masked_teacher.argmax(dim=-1)
        ce_loss = F.cross_entropy(masked_logits, hard_labels)

        # Combined loss
        total_loss = 0.85 * kl_loss + 0.15 * ce_loss

        return total_loss


# ============================================================================
# PART 5: TEXT GENERATION
# ============================================================================

@torch.no_grad()
def generate_text(model: nn.Module, tokenizer: Any, prompt: str,
                  max_length: int = 100, temperature: float = 0.8,
                  top_p: float = 0.9, device: torch.device = DEVICE):
    """Generate text using the model"""

    model.eval()

    # Tokenize prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)

    # Generate
    generated = input_ids

    for _ in range(max_length):
        # Forward pass
        logits, _ = model(generated)
        next_token_logits = logits[0, -1, :] / temperature

        # Top-p sampling
        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

        # Remove tokens with cumulative probability above the threshold
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0

        indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)
        next_token_logits[indices_to_remove] = -float('Inf')

        # Sample
        probs = F.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        # Append
        generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)

        # Stop if EOS
        if next_token.item() == tokenizer.eos_token_id:
            break

    # Decode
    text = tokenizer.decode(generated[0], skip_special_tokens=True)
    return text


# ============================================================================
# PART 6: MAIN EXECUTION
# ============================================================================

def main():
    """Main training pipeline"""

    print("\n" + "="*70)
    print("RWKV-7 Training Pipeline - Starting")
    print("="*70)

    # Configuration
    TEACHER_MODEL = "Qwen/Qwen3-0.6B"  # Or any small model
    STUDENT_CONFIG = {
        'hidden_size': 1024,
        'num_layers': 12,
        'num_heads': 8
    }

    # Load dataset
    # Try different datasets in order of preference
    dataset_options = [
        ("tatsu-lab/alpaca", 10000),  # Alpaca - instruction following
        ("roneneldan/TinyStories", 10000),  # TinyStories - simple narratives
        ("wikitext", 10000),  # WikiText - general text
    ]

    dataset = None
    for dataset_name, max_samples in dataset_options:
        try:
            dataset = load_and_prepare_dataset(dataset_name, max_samples)
            print(f"Successfully loaded {dataset_name}")
            break
        except Exception as e:
            print(f"Failed to load {dataset_name}: {e}")
            continue

    if dataset is None:
        print("ERROR: Could not load any dataset. Using dummy data.")
        dataset = SimpleTextDataset(["Dummy text"] * 100)

    # Initialize trainer
    trainer = DistillationTrainer(
        teacher_model_name=TEACHER_MODEL,
        student_config=STUDENT_CONFIG,
        device=DEVICE
    )

    # Train model
    trained_model = trainer.train(
        dataset,
        num_epochs=2,
        batch_size=4,
        lr=5e-4,
        max_length=256,
        gradient_accumulation_steps=2,
        checkpoint_dir="checkpoints"
    )

    # Save final model
    final_path = "rwkv7_final_model.pt"
    torch.save(trained_model.state_dict(), final_path)
    print(f"\n{'='*60}")
    print(f"Training completed! Model saved to: {final_path}")
    print(f"{'='*60}")

    # Test generation
    print("\n" + "="*60)
    print("Testing text generation...")
    print("="*60)

    test_prompts = [
        "The capital of France is",
        "Machine learning is",
        "### Instruction:\nWhat is Python?\n\n### Response:\n",
    ]

    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        generated = generate_text(
            trained_model,
            trainer.tokenizer,
            prompt,
            max_length=50,
            temperature=0.8,
            device=DEVICE
        )
        print(f"Generated: {generated}")
        print("-" * 40)

    return trained_model, trainer


# ============================================================================
# PART 7: RUN THE PIPELINE
# ============================================================================

if __name__ == "__main__":
    # Run the complete pipeline
    model, trainer = main()

    print("\n" + "="*70)
    print("Pipeline completed successfully!")
    print("="*70)

    # Interactive mode
    print("\nEntering interactive mode. Type 'quit' to exit.")
    while True:
        prompt = input("\nEnter prompt: ")
        if prompt.lower() == 'quit':
            break

        generated = generate_text(
            model,
            trainer.tokenizer,
            prompt,
            max_length=100,
            device=DEVICE
        )
        print(f"\nGenerated:\n{generated}")
