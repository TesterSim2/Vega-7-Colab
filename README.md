# Vega-7-Colab
We're building upon the Arxiv paper â€œARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer), in which the whole idea is swapping the parts of a pre-existing open source transformer model and performing a distillation pipeline to convert a Transformer model into a RWKV-7 model.
